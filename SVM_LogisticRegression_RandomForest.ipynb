{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import emoji\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from io import StringIO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    \n",
    "    def split_data(self,X, Y, percentage=0.8):\n",
    "        \"\"\"\n",
    "         Split the training data into training and test set according to given percentage... \n",
    "\n",
    "        Parameters:\n",
    "        --------\n",
    "        X: training examples\n",
    "        Y: training labels\n",
    "        percentage: split data into train and test accorind to given %\n",
    "\n",
    "        Returns:\n",
    "        ---------    \n",
    "        returns four lists as tuple: training data, training labels, test data, test labels \n",
    "        \"\"\"\n",
    "\n",
    "        testp=1-percentage\n",
    "\n",
    "        #Split the data into train and test according to given fraction..\n",
    "\n",
    "        #Creat a list of tuples according to the n-classes where each tuple will \n",
    "        # contain the pair of training and test examples for that class...\n",
    "        #each tuple=(training-examples, training-labels,testing-examples,testing-labels)\n",
    "        exdata=[]\n",
    "        #Creat 4 different lists \n",
    "        traindata=[]\n",
    "        trainlabels=[]\n",
    "        testdata=[]\n",
    "        testlabels=[]\n",
    "\n",
    "        classes=np.unique(Y)\n",
    "\n",
    "        for c in classes:\n",
    "            # print c\n",
    "            idx=Y==c\n",
    "            Yt=Y[idx]\n",
    "            Xt=X[idx,:]\n",
    "            nexamples=Xt.shape[0]\n",
    "            # Generate a random permutation of the indeces\n",
    "            ridx=np.arange(nexamples) # generate indeces\n",
    "            np.random.shuffle(ridx)\n",
    "            ntrainex=round(nexamples*percentage)\n",
    "            ntestex=nexamples-ntrainex\n",
    "\n",
    "            ntrainex = int(ntrainex)\n",
    "\n",
    "            traindata.append(Xt[ridx[:ntrainex],:])\n",
    "            trainlabels.append(Yt[ridx[:ntrainex]])\n",
    "\n",
    "            testdata.append(Xt[ridx[ntrainex:],:])\n",
    "            testlabels.append(Yt[ridx[ntrainex:]])\n",
    "\n",
    "            #exdata.append((Xt[ridx[:ntrainex],:], Yt[ridx[:ntrainex]], Xt[ridx[ntrainex:],:], Yt[ridx[ntrainex:]]))\n",
    "\n",
    "\n",
    "        # print traindata,trainlabels\n",
    "        Xtrain=np.concatenate(traindata)\n",
    "        Ytrain=np.concatenate(trainlabels)\n",
    "        Xtest=np.concatenate(testdata)\n",
    "        Ytest=np.concatenate(testlabels)\n",
    "        return Xtrain, Ytrain, Xtest, Ytest\n",
    "    \n",
    "    def loadData(self, filename):\n",
    "        self.data = pd.read_csv(filename,sep='\\t')\n",
    "    \n",
    "    def removeDuplicatesColumns(self):\n",
    "        self.data   = self.data.drop_duplicates(subset='tweet_id')\n",
    "        self.t_data = self.data[['text','sentiment','tweet_id']]\n",
    "    \n",
    "    def split(self):\n",
    "        \n",
    "        x = self.t_data[\"text\"].values\n",
    "        X = x.reshape((x.shape[0],1))\n",
    "        Y = np.array(self.t_data['sentiment'])\n",
    "        Xtrain, Ytrain, Xtest, Ytest = self.split_data(X,Y)\n",
    "        \n",
    "        \n",
    "        train=pd.DataFrame(Xtrain,columns=['text'])\n",
    "        train['sentiment']=Ytrain\n",
    "        self.train=train\n",
    "        \n",
    "        test=pd.DataFrame(Xtest,columns=['text'])\n",
    "        test['sentiment']=Ytest\n",
    "        self.test=test\n",
    "        \n",
    "    def mark_neg(self, tweet):\n",
    "        \n",
    "        tk=nltk.casual.TweetTokenizer()\n",
    "        doc=tk.tokenize(tweet)\n",
    "        pos=nltk.pos_tag(doc)\n",
    "\n",
    "\n",
    "        flag=False\n",
    "\n",
    "        for i in range(0, len(pos)-2):\n",
    "            if (pos[i][0]=='not' or (\"n't\" in pos[i][0]) or pos[i][0]==\"no\"):\n",
    "                #print ('oye hoye0')\n",
    "                flag = not flag\n",
    "            if flag==True:\n",
    "                #if (pos[i][1]=='JJ' or pos[i][1]=='JJR' or pos[i][1]=='JJS') or (pos[i][1]=='VB' or pos[i][1]=='VBD' or pos[i][1]=='VBG' or pos[i][1]=='VBN' or pos[i][1]=='VBP' or pos[i][1]=='VBZ') or (pos[i][1]=='NN' or pos[i][1]=='NNS' or pos[i][1]=='NNP' or pos[i][1]=='NNPS'):\n",
    "                    #print ('oye hoye1')\n",
    "                #    doc[i]='_'.join(['not',doc[i]])\n",
    "                if ((pos[i][1]=='RB' or pos[i][1]=='RBR' or pos[i][1]=='RBS') or pos[i][1]=='DT') and (pos[i+1][1]=='JJ' or pos[i+1][1]=='JJR' or pos[i+1][1]=='JJS'):\n",
    "                    #print ('oye hoye2')\n",
    "                    doc[i+1]='_'.join(['not',doc[i+1]])\n",
    "                    \n",
    "                if pos[i][1]=='DT' and (pos[i+1][1]=='RB' or pos[i+1][1]=='RBR' or pos[i+1][1]=='RBS') and (pos[i+2][1]=='JJ' or pos[i+2][1]=='JJR' or pos[i+2][1]=='JJS'):\n",
    "                    #print ('oye hoye3')\n",
    "                    doc[i+2]='_'.join(['not',doc[i+2]])\n",
    "                \n",
    "                flag=False\n",
    "\n",
    "        return ' '.join(doc)\n",
    "    \n",
    "    def custom_mark_neg(self, tweet):\n",
    "        \n",
    "        tk=nltk.casual.TweetTokenizer()\n",
    "        doc=tk.tokenize(tweet)\n",
    "        pos=nltk.pos_tag(doc)\n",
    "\n",
    "\n",
    "        flag=False\n",
    "\n",
    "        for i in range(0, len(pos)):\n",
    "            if (pos[i][0]=='not' or (\"n't\" in pos[i][0]) or pos[i][0]==\"no\"):\n",
    "                #print ('oye hoye0')\n",
    "                flag = not flag\n",
    "            if flag==True:\n",
    "                \n",
    "                if i+1<len(pos):\n",
    "                    if pos[i+1][1]=='JJ' or pos[i+1][1]=='JJR' or pos[i+1][1]=='JJS':\n",
    "                        #doc[i+1]='_'.join(['not',doc[i+1]])\n",
    "                        doc[i+1]=getAntonym(doc[i+1])\n",
    "                        doc[i]=''\n",
    "                    elif ((pos[i][1]=='RB' or pos[i][1]=='RBR' or pos[i][1]=='RBS') or pos[i][1]=='DT') and (pos[i+1][1]=='JJ' or pos[i+1][1]=='JJR' or pos[i+1][1]=='JJS'):\n",
    "                        #doc[i+1]='_'.join(['not',doc[i+1]])\n",
    "                        doc[i+1]=getAntonym(doc[i+1])\n",
    "                        doc[i]=''\n",
    "                elif i+2<len(pos):\n",
    "                    if pos[i+2][1]=='JJ' or pos[i+2][1]=='JJR' or pos[i+2][1]=='JJS':\n",
    "                        #doc[i+2]='_'.join(['not',doc[i+2]])\n",
    "                        doc[i+2]=getAntonym(doc[i+2])\n",
    "                        doc[i]=''\n",
    "                    elif pos[i][1]=='DT' and (pos[i+1][1]=='RB' or pos[i+1][1]=='RBR' or pos[i+1][1]=='RBS') and (pos[i+2][1]=='JJ' or pos[i+2][1]=='JJR' or pos[i+2][1]=='JJS'):\n",
    "                        #doc[i+2]='_'.join(['not',doc[i+2]])\n",
    "                        doc[i+2]=getAntonym(doc[i+2])\n",
    "                        doc[i]=''\n",
    "                elif i+3<len(pos):\n",
    "                    if pos[i+3][1]=='JJ' or pos[i+3][1]=='JJR' or pos[i+3][1]=='JJS':\n",
    "                        #doc[i+3]='_'.join(['not',doc[i+3]])\n",
    "                        doc[i+3]=getAntonym(doc[i+3])\n",
    "                        doc[i]=''\n",
    "                elif i+4<len(pos):\n",
    "                    if pos[i+4][1]=='JJ' or pos[i+4][1]=='JJR' or pos[i+4][1]=='JJS':\n",
    "                        #doc[i+4]='_'.join(['not',doc[i+4]])\n",
    "                        doc[i+4]=getAntonym(doc[i+4])\n",
    "                        doc[i]=''\n",
    "               \n",
    "                \n",
    "                flag=False\n",
    "\n",
    "        return ' '.join(doc)\n",
    "    \n",
    "        \n",
    "        \n",
    "    def give_emoji_free_text(self, text):\n",
    "        allchars   = [str for str in text]\n",
    "        emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "        clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U0001F1F2-\\U0001F1F4\"  # Macau flag\n",
    "            u\"\\U0001F1E6-\\U0001F1FF\"  # flags\n",
    "            u\"\\U0001F600-\\U0001F64F\"\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u\"\\U0001F1F2\"\n",
    "            u\"\\U0001F1F4\"\n",
    "            u\"\\U0001F620\"\n",
    "            u\"\\u200d\"\n",
    "            u\"\\u2640-\\u2642\"\n",
    "            \"]+\", flags=re.UNICODE)\n",
    "\n",
    "        clean_text = emoji_pattern.sub(r'', clean_text)\n",
    "\n",
    "        return clean_text\n",
    "\n",
    "    def preProcessingSubfunction(self, tweet):\n",
    "        tweet=tweet.encode().decode(\"utf-8-sig\")\n",
    "        tweet=tweet.replace(u\"\\ufffd\", \"?\")\n",
    "        tweet=tweet.replace(u\"\\\\u002c\",\"\")\n",
    "        tweet=re.sub('\\s+', ' ', tweet)\n",
    "        \n",
    "        tweet=re.sub(r'[0-9]+(th|nd|st|rd)','',tweet)\n",
    "        \n",
    "        tweet=re.sub('@[A-Za-z0-9_]+|https?://[^ ]+|^(RT )|( RT )', ' ', tweet)\n",
    "        tweet=re.sub('www.[^ ]+', ' ', tweet)\n",
    "        tweet=BeautifulSoup(tweet, 'lxml').get_text().lower()\n",
    "        tweet=re.sub(r\"\"\"[\"\\?,$!@#\\-$\\\\%\\^&*()\\[\\]{}`“\\/~\\|._\\+\\-;<>=:]|'(?!(?<! ')[ts])\"\"\", \"\", tweet)\n",
    "        #tweet=self.give_emoji_free_text(tweet)\n",
    "        tweet = apostrapheHandler(tweet)\n",
    "        #hash removal STARTS\n",
    "        #tweet=re.sub(r'#[a-z0-9]*','',tweet)\n",
    "        #hash removal ENDS\n",
    "        \n",
    "        #acronyms STARTS\n",
    "        tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)   \n",
    "        tweet = self.acronymHandler(tweet)\n",
    "        #acronyms ENDS\n",
    "        \n",
    "        tweet=re.sub(r\"[0-9]\",\"\",tweet)\n",
    "        \n",
    "        #tweet=\" \".join(mark_negation(tweet.split()))\n",
    "        #tweet=self.mark_neg(tweet)\n",
    "        tweet=self.custom_mark_neg(tweet)\n",
    "        \n",
    "        return tweet\n",
    " \n",
    "    def acronymHandler(self, tweet):\n",
    "        tokenizer = nltk.casual.TweetTokenizer()\n",
    "        tweetArr = tokenizer.tokenize(tweet)\n",
    "        for i in range(len(tweetArr)):\n",
    "            #print(tweetArr[i])\n",
    "            if tweetArr[i] in self.acrDict:\n",
    "                tweetArr[i] = self.acrDict[tweetArr[i]]\n",
    "        \n",
    "    \n",
    "        tweet = ' '.join(tweetArr)\n",
    "        return tweet\n",
    "    \n",
    "    def makeAcronymDictionary(self,filename):\n",
    "        acrData = pd.read_csv(filename,sep=\"\\t\")\n",
    "        acrData = acrData.apply(lambda x: x.astype(str).str.lower())\n",
    "        self.acrDict = dict(zip(acrData.acronym, acrData.definition))\n",
    "\n",
    "    def preProcessing(self):\n",
    "        self.makeAcronymDictionary(\"acronyms.txt\")\n",
    "        self.train['processed_text'] = self.train['text'].apply(lambda x: self.preProcessingSubfunction(x))\n",
    "        self.test ['processed_text'] = self.test['text'].apply(lambda x: self.preProcessingSubfunction(x))\n",
    "    \n",
    "    def align(self, arr=['text','processed_text','sentiment']):\n",
    "        self.train=self.train[arr]\n",
    "        self.test =self.test[arr]\n",
    "    \n",
    "    def targetDomain(self):\n",
    "        return list(np.unique(self.train['sentiment']))\n",
    "    \n",
    "    def testLabels(self):\n",
    "        return list(self.test['sentiment'])\n",
    "    \n",
    "    def underSampling(self):\n",
    "        \n",
    "        cnames=['Positive','Negative','Neutral']\n",
    "        classes=set([0,1,2])\n",
    "        PosNegNeu=[(sum(self.t_data['sentiment']=='Positive')),(sum(self.t_data['sentiment']=='Negative')),(sum(self.t_data['sentiment']=='Neutral'))]\n",
    "        minone=np.argmin(PosNegNeu)\n",
    "\n",
    "\n",
    "        new_t_data = self.t_data[(self.t_data['sentiment']==cnames[minone])]\n",
    "\n",
    "        classes=classes-set([minone])\n",
    "        for _class in classes:\n",
    "            _class_indices = self.t_data[self.t_data.sentiment == cnames[_class]].index\n",
    "            random_indices = np.random.choice(_class_indices, sum(self.t_data['sentiment']==cnames[minone]), replace=False)\n",
    "            _class_sample  = self.t_data.loc[random_indices]\n",
    "\n",
    "            new_t_data = pd.concat([new_t_data, _class_sample], ignore_index=True)\n",
    "\n",
    "        self.t_data=new_t_data\n",
    "        self.t_data=self.t_data.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "  \n",
    "    def train_classifier(self, tokenizer=nltk.casual.TweetTokenizer(),vectorizer=CountVectorizer(),mindif=2,mfeatures=None,stopwords=None,classifier=MultinomialNB(),ngramRng=(1,1)):\n",
    "        # initialize tweet_vector object, and then turn tweet train data into a vector \n",
    "        self.tokenizer  = tokenizer\n",
    "        self.vectorizer = vectorizer        \n",
    "        self.vectorizer.set_params(min_df=mindif, tokenizer=self.tokenizer.tokenize,max_features=mfeatures, stop_words=stopwords,ngram_range=ngramRng)\n",
    "\n",
    "        self.train_tweet_counts = self.vectorizer.fit_transform(self.train['processed_text'])\n",
    "        self.classifier = classifier.fit(self.train_tweet_counts, list(self.train['sentiment']))\n",
    "\n",
    "    def predict(self):\n",
    "        \n",
    "        test_tweet_counts = self.vectorizer.transform(self.test['processed_text'])\n",
    "        return self.classifier.predict(test_tweet_counts)\n",
    "        \n",
    "def printings():\n",
    "    labels = naivebayes.targetDomain()\n",
    "    conmat = np.array(confusion_matrix(naivebayes.testLabels(), y_pred, labels=labels))\n",
    "    confusion = pd.DataFrame(conmat, index=labels,\n",
    "                             columns=['predicted_'+x for x in labels])\n",
    "    print (\"\\n\")\n",
    "    print (\"Accuracy Score: {0:.2f}%\".format(accuracy_score(naivebayes.testLabels(), y_pred)*100))\n",
    "    print (\"-\"*80)\n",
    "    print (\"Confusion Matrix\\n\")\n",
    "    print (confusion)\n",
    "    print (\"-\"*80)\n",
    "    print (\"Classification Report\\n\")\n",
    "    print (classification_report(naivebayes.testLabels(), y_pred,digits=5))\n",
    "\n",
    "def getCustomStopWords():\n",
    "    stopWords=set(stopwords.words('english'))\n",
    "\n",
    "    _stopWords=[]\n",
    "    for x in stopWords:\n",
    "        if \"'\" not in x:\n",
    "            _stopWords.append(x)\n",
    "    return _stopWords\n",
    "\n",
    "def getCustomStopWords3():\n",
    "    stopWords=set(stopwords.words('english'))\n",
    "    li=[]\n",
    "\n",
    "    oldValue=report_to_df(classification_report(naivebayes.testLabels(), y_pred,digits=20))['f1-score']['avg/total']\n",
    "\n",
    "    for a in stopWords:\n",
    "        fake=li.copy()\n",
    "        fake.append(a)\n",
    "\n",
    "        naivebayes.train_classifier(stopwords=set(fake),mindif=3,ngramRng=(1,5),vectorizer=TfidfVectorizer())\n",
    "        y_pred = naivebayes.predict()\n",
    "\n",
    "        newValue=report_to_df(classification_report(naivebayes.testLabels(), y_pred,digits=20))['f1-score']['avg/total']\n",
    "   \n",
    "        if oldValue < newValue:\n",
    "            li.append(a)\n",
    "            oldValue=newValue\n",
    "            \n",
    "    return li\n",
    "\n",
    "\n",
    "def getCustomStopWords2():\n",
    "    stopWords=set(stopwords.words('english'))\n",
    "\n",
    "    _stopWords=[]\n",
    "    for x in stopWords:\n",
    "        if \"not\" in x or \"n't\" in x or \"no\" in x:\n",
    "            _stopWords.append(x)\n",
    "    return _stopWords\n",
    "\n",
    "def apostrapheHandler(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"let\\'s\", \"let us\", phrase)\n",
    "    phrase = re.sub(r\"shan\\'t\", \"shall not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def report_to_df(report):\n",
    "    report = re.sub(r\" +\", \" \", report).replace(\"avg / total\", \"avg/total\").replace(\"\\n \", \"\\n\")\n",
    "    report_df = pd.read_csv(StringIO(\"Classes\" + report), sep=' ', index_col=0)        \n",
    "    return(report_df)\n",
    "\n",
    "def getAntonym(word):\n",
    "\n",
    "    customDict={'alive':'dead',\n",
    "\n",
    "    'backward':'forward',\n",
    "\n",
    "    'beautiful':'ugly',\n",
    "\n",
    "    'big':'small',\n",
    "\n",
    "    'blunt':'sharp',\n",
    "\n",
    "    'boring':'interesting',\n",
    "\n",
    "    'bright' :'dark' ,\n",
    "\n",
    "    'broad':'narrow',\n",
    "\n",
    "    'clean':'dirty',\n",
    "\n",
    "    'intelligent':'stupid',\n",
    "\n",
    "    'closed':'open',\n",
    "\n",
    "    'cool':'warm',\n",
    "\n",
    "    'cruel':'kind',\n",
    "\n",
    "    'dangerous':'safe',\n",
    "\n",
    "\n",
    "\n",
    "    'deep':'shallow',\n",
    "\n",
    "    'difficult':'easy',\n",
    "\n",
    "    'dry':'wet',\n",
    "\n",
    "    'early':'late',\n",
    "\n",
    "    'fake':'real',\n",
    "\n",
    "    'fast':'slow',\n",
    "\n",
    "    'fat':'thin',\n",
    "\n",
    "    'gentle':'fierce',\n",
    "\n",
    "    'good':'bad',\n",
    "\n",
    "    'happy':'sad',\n",
    "\n",
    "    'hard':'soft',\n",
    "\n",
    "    'heavy':'light' ,\n",
    "\n",
    "    'high':'low',\n",
    "\n",
    "    'hot':'cold',\n",
    "\n",
    "    'ill':'well',\n",
    "\n",
    "    'innocent':'guilty',\n",
    "\n",
    "    'long' :'short' ,\n",
    "\n",
    "    'loose':'tight',\n",
    "\n",
    "    'loud' :'soft' ,\n",
    "\n",
    "    'low':'high',\n",
    "\n",
    "    'modern':'ancient',\n",
    "\n",
    "    'noisy':'quiet',\n",
    "\n",
    "    'normal':'strange',\n",
    "\n",
    "    'old' :'new' ,\n",
    "\n",
    "    'outgoing':'shy',\n",
    "\n",
    "    'poor':'rich',\n",
    "\n",
    "    'right' :'wrong',\n",
    "\n",
    "\n",
    "    'rough':'smooth',\n",
    "\n",
    "    'short' :'tall' ,\n",
    "\n",
    "    'sour':'sweet',\n",
    "\n",
    "    'strong':'weak',\n",
    "\n",
    "    'terrible':'wonderful',\n",
    "\n",
    "    'far':'near',\n",
    "\n",
    "    'cheap':'expensive',\n",
    "    \n",
    "    'ok':'disapprove'\n",
    "            \n",
    "    }\n",
    "    \n",
    "    if word in customDict.keys():\n",
    "        return customDict[word]\n",
    "    else:\n",
    "        antonyms = [] \n",
    "\n",
    "        \n",
    "        for syn in wordnet.synsets(word): \n",
    "            for l in syn.lemmas(): \n",
    "                if l.antonyms(): \n",
    "                    antonyms.append(l.antonyms()[0].name()) \n",
    "\n",
    "\n",
    "        counter=Counter(antonyms) \n",
    "        if len(counter)>=1:\n",
    "            return counter.most_common(1)[0][0]\n",
    "        else:\n",
    "            return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "naivebayes = Classifier()\n",
    "naivebayes.loadData('Custom.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This time NEXT SATURDAY NIGHT we will be about...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>224057065746862080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Notes on the Republican Debate (gibberish) htt...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>629674653947047936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Moving out by January and buying a new Lexus, ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>637375640472190976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Starting to make our wicker  lanterns today fo...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>255225557673967616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Every time the debate bell rings, my dogs frea...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>629479907807129600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment  \\\n",
       "0  This time NEXT SATURDAY NIGHT we will be about...   Neutral   \n",
       "1  Notes on the Republican Debate (gibberish) htt...  Negative   \n",
       "2  Moving out by January and buying a new Lexus, ...  Positive   \n",
       "3  Starting to make our wicker  lanterns today fo...   Neutral   \n",
       "4  Every time the debate bell rings, my dogs frea...   Neutral   \n",
       "\n",
       "             tweet_id  \n",
       "0  224057065746862080  \n",
       "1  629674653947047936  \n",
       "2  637375640472190976  \n",
       "3  255225557673967616  \n",
       "4  629479907807129600  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naivebayes.data=naivebayes.data[~naivebayes.data['text'].apply(lambda x: (('neutral' in x) or ('positive' in x) or ('negative' in x)) and (len(x) > 140) )]\n",
    "naivebayes.removeDuplicatesColumns()\n",
    "naivebayes.t_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29336\n"
     ]
    }
   ],
   "source": [
    "print(len(naivebayes.t_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "naivebayes.makeAcronymDictionary(\"acronyms.txt\")\n",
    "naivebayes.t_data['processed_text'] = naivebayes.t_data['text'].apply(lambda x: naivebayes.preProcessingSubfunction(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features = vectorizer.fit_transform(naivebayes.t_data['processed_text']).toarray()\n",
    "labels = naivebayes.t_data['sentiment']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "N = 2 \n",
    "for s in (np.unique(naivebayes.train['sentiment'])):\n",
    "    features_chi2 = chi2(features,labels == s)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(vectorizer.get_feature_names())[indices]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=10\n",
    "unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "print(\"# '{}':\".format(\"Positive\"))\n",
    "print(\"  . Most correlated unigrams:\\n       . {}\".format('\\n       . '.join(unigrams[-N:])))\n",
    "print(\"  . Most correlated bigrams:\\n       . {}\".format('\\n       . '.join(bigrams[-N:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6651556328889567\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "#0.6655646656997667\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True,norm=\"l2\",min_df=4,ngram_range=(1,2))\n",
    "features = vectorizer.fit_transform(naivebayes.t_data['processed_text']).toarray()\n",
    "\n",
    "models = [\n",
    "   \n",
    "    LogisticRegression(random_state=0),\n",
    "\n",
    "]\n",
    "\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "entries = []\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "print (cv_df['accuracy'].mean())\n",
    "\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats of Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic:  0.6495432263620455\n",
    "svm:  0.6625648768097276\n",
    "mindf:  1\n",
    "gramrng:  1\n",
    "\n",
    "\n",
    "logistic:  0.6447025690699624\n",
    "svm:  0.6642692615183023\n",
    "mindf:  2\n",
    "gramrng:  1\n",
    "\n",
    "\n",
    "logistic:  0.6443959193618042\n",
    "svm:  0.6635534221482351\n",
    "mindf:  3\n",
    "gramrng:  1\n",
    "\n",
    "\n",
    "logistic:  0.6465093078346892\n",
    "svm:  0.6638602403260923\n",
    "mindf:  4\n",
    "gramrng:  1\n",
    "\n",
    "\n",
    "logistic:  0.6461344220890963\n",
    "svm:  0.6629739502856374\n",
    "mindf:  5\n",
    "gramrng:  1\n",
    "\n",
    "logistic:  0.6511455590980668\n",
    "svm:  0.6645761087426596\n",
    "mindf:  2\n",
    "gramrng:  2\n",
    "\n",
    "\n",
    "logistic:  0.6472935575211902\n",
    "svm:  0.6646442634498946\n",
    "mindf:  3\n",
    "gramrng:  2\n",
    "\n",
    "\n",
    "logistic:  0.6449757049266001\n",
    "svm:  0.6655646656997667\n",
    "mindf:  4\n",
    "gramrng:  2\n",
    "\n",
    "\n",
    "logistic:  0.6456915559152673\n",
    "svm:  0.6649851996464692\n",
    "mindf:  5\n",
    "gramrng:  2\n",
    "\n",
    "\n",
    "logistic:  0.650940978790362\n",
    "svm:  0.6634511668501825\n",
    "mindf:  2\n",
    "gramrng:  3\n",
    "\n",
    "\n",
    "logistic:  0.6466459077141578\n",
    "svm:  0.6638261397352748\n",
    "mindf:  3\n",
    "gramrng:  3\n",
    "\n",
    "\n",
    "logistic:  0.6452481843323412\n",
    "svm:  0.6647806948596642\n",
    "mindf:  4\n",
    "gramrng:  3\n",
    "\n",
    "\n",
    "logistic:  0.6438508385550226\n",
    "svm:  0.6647806716224643\n",
    "mindf:  5\n",
    "gramrng:  3\n",
    "\n",
    "\n",
    "logistic:  0.6510431818047148\n",
    "svm:  0.6628034938059502\n",
    "mindf:  2\n",
    "gramrng:  4\n",
    "    \n",
    "    logistic:  0.6467481920587101\n",
    "svm:  0.6642693021834022\n",
    "mindf:  3\n",
    "gramrng:  4\n",
    "\n",
    "\n",
    "logistic:  0.6449414707218833\n",
    "svm:  0.6649170565578341\n",
    "mindf:  4\n",
    "gramrng:  4\n",
    "\n",
    "\n",
    "logistic:  0.6432031074177906\n",
    "svm:  0.6642693602764018\n",
    "mindf:  5\n",
    "gramrng:  4\n",
    "\n",
    "\n",
    "logistic:  0.6460323004049431\n",
    "svm:  0.6645760855054597\n",
    "mindf:  3\n",
    "gramrng:  5\n",
    "\n",
    "\n",
    "logistic:  0.6444301884223208\n",
    "svm:  0.6649511222928517\n",
    "mindf:  4\n",
    "gramrng:  5\n",
    "\n",
    "\n",
    "logistic:  0.6433394516880606\n",
    "svm:  0.6645420720541417\n",
    "mindf:  5\n",
    "gramrng:  5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
